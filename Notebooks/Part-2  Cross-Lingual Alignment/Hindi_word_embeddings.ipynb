{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05aca86",
   "metadata": {},
   "source": [
    "# Hindi Word Embeddings: Complete Pipeline\n",
    "\n",
    "This notebook implements a complete pipeline for creating and evaluating English word embeddings using frequency-based co-occurrence matrices and PCA dimensionality reduction.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Data Preprocessing**: Text cleaning, tokenization, vocabulary building\n",
    "2. **Co-occurrence Matrix Construction**: Frequency-based approach with window size experimentation\n",
    "3. **Dimensionality Reduction**: PCA with dimension experimentation\n",
    "4. **Quantitative Evaluation**: Covariance, cosine similarity, analogies, clustering\n",
    "5. **Visualization**: t-SNE, PCA, similarity heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd85d2",
   "metadata": {},
   "source": [
    "## üîß Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb74a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting indic-nlp-library\n",
      "  Using cached indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\n",
      "  Using cached sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
      "  Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting morfessor (from indic-nlp-library)\n",
      "  Using cached Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from indic-nlp-library) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from indic-nlp-library) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->indic-nlp-library) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->indic-nlp-library) (2023.3)\n",
      "Requirement already satisfied: sphinx>=5.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx-argparse->indic-nlp-library) (7.3.7)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
      "  Using cached docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.15.1)\n",
      "Requirement already satisfied: snowballstemmer>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.11.0)\n",
      "Requirement already satisfied: alabaster~=0.7.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
      "Requirement already satisfied: imagesize>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
      "Requirement already satisfied: colorama>=0.4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.8.30)\n",
      "Using cached indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
      "Using cached Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Using cached sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
      "Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
      "Using cached docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Installing collected packages: morfessor, docutils, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
      "Successfully installed docutils-0.21.2 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts docutils.exe, rst2html.exe, rst2html4.exe, rst2html5.exe, rst2latex.exe, rst2man.exe, rst2odt.exe, rst2pseudoxml.exe, rst2s5.exe, rst2xetex.exe and rst2xml.exe are installed in 'C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3d6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indicnlp\n",
    "from indicnlp import common\n",
    "from indicnlp import loader\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45a24e",
   "metadata": {},
   "source": [
    "We will explore the generation of dense word representations from text corpora, analyze the quality of these representations. The goal is to understand how to represent word meaning in a high-dimensional space and how to transfer knowledge across languages.\n",
    "\n",
    "Please use the links below to download text corpora. \\\n",
    "English: https://wortschatz.uni-leipzig.de/en/download/English \\\n",
    "Hindi: https://wortschatz.uni-leipzig.de/en/download/Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6263f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_43680\\3395577974.py:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  path = \"D:\\PROJECTS\\PreCog\\Data\\hin_news_2020_300K\\hin_news_2020_300K-sentences.txt\"\n"
     ]
    }
   ],
   "source": [
    "# Set up Indic NLP resources\n",
    "INDIC_RESOURCES_PATH = \"D:\\\\RESEARCH related\\\\PreCog tasks\\\\indic_nlp_resources\"  # Replace with your path\n",
    "common.set_resources_path(INDIC_RESOURCES_PATH)\n",
    "loader.load()\n",
    "\n",
    "# Load Hindi corpus file\n",
    "path = \"D:\\PROJECTS\\PreCog\\Data\\hin_news_2020_300K\\hin_news_2020_300K-sentences.txt\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    hindi_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cabd97",
   "metadata": {},
   "source": [
    "## Preprocessing of data\n",
    "1. normalize\n",
    "2. replace new lines with spaces\n",
    "3. Remove non-alphabetic characters (removing punctuations)\n",
    "4. how about stop words removal --> they impact hugely in co occurence matrices so don't do it.\n",
    "5. lemitization & stemming  (does this helpful for this task)\n",
    "6. Tokenize words after all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2295c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 3\t‡•¶ ‡§Æ‡•á‡§Ç ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§≤‡•â‡§ï‡§°‡§æ‡§â‡§® ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§ó‡§∞‡•Ä‡§¨ ‡§ï‡§≤‡•ç‡§Ø‡§æ‡§£ ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡§æ ‡§ê‡§≤‡§æ‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§\n",
      "\n",
      "Processed: ['3', '‡•¶', '‡§Æ‡•á‡§Ç', '‡§ï‡§π‡§æ', '‡§ï‡§ø', '‡§≤‡•â‡§ï‡§°‡§æ‡§â‡§®', '‡§ï‡•á', '‡§¨‡§æ‡§¶', '‡§ó‡§∞‡•Ä‡§¨', '‡§ï‡§≤‡•ç‡§Ø‡§æ‡§£', '‡§Ø‡•ã‡§ú‡§®‡§æ', '‡§ï‡§æ', '‡§ê‡§≤‡§æ‡§®', '‡§ï‡§ø‡§Ø‡§æ', '‡§ó‡§Ø‡§æ', '‡§•‡§æ', '‡•§']\n",
      "\n",
      "Original: 4\t\"100 ‡§Æ‡§∞‡•Ä‡§ú‡•ã‡§Ç ‡§™‡§∞ ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡•ç‡§≤‡§ø‡§®‡§ø‡§ï‡§≤ ‡§ü‡•ç‡§∞‡§æ‡§Ø‡§≤ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ, ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§§‡•Ä‡§® ‡§¶‡§ø‡§® ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ 69 ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§î‡§∞ ‡§ö‡§æ‡§∞ ‡§¶‡§ø‡§® ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§∂‡§§ ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§Æ‡§∞‡•Ä‡§ú ‡§†‡•Ä‡§ï ‡§π‡•ã ‡§ó‡§è ‡§î‡§∞ ‡§â‡§®‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü ‡§®‡§ø‡§ó‡•á‡§ü‡§ø‡§µ ‡§Ü‡§à‡•§\"\n",
      "\n",
      "Processed: ['4', '\"', '100', '‡§Æ‡§∞‡•Ä‡§ú‡•ã‡§Ç', '‡§™‡§∞', '‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§', '‡§ï‡•ç‡§≤‡§ø‡§®‡§ø‡§ï‡§≤', '‡§ü‡•ç‡§∞‡§æ‡§Ø‡§≤', '‡§ï‡§ø‡§Ø‡§æ', '‡§ó‡§Ø‡§æ', ',', '‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç', '‡§§‡•Ä‡§®', '‡§¶‡§ø‡§®', '‡§ï‡•á', '‡§Ö‡§Ç‡§¶‡§∞', '69', '‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§', '‡§î‡§∞', '‡§ö‡§æ‡§∞', '‡§¶‡§ø‡§®', '‡§ï‡•á', '‡§Ö‡§Ç‡§¶‡§∞', '‡§∂‡§§', '‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§', '‡§Æ‡§∞‡•Ä‡§ú', '‡§†‡•Ä‡§ï', '‡§π‡•ã', '‡§ó‡§è', '‡§î‡§∞', '‡§â‡§®‡§ï‡•Ä', '‡§ú‡§æ‡§Ç‡§ö', '‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü', '‡§®‡§ø‡§ó‡•á‡§ü‡§ø‡§µ', '‡§Ü‡§à', '‡•§', '\"']\n",
      "\n",
      "Original: 5\t'100 ‡§Æ‡•á‡§Ç 70 ‡§Ö‡§´‡§º‡§∏‡§∞ ‡§¨‡§®‡§®‡•á ‡§≤‡§æ‡§Ø‡§ï‡§º ‡§®‡§π‡•Ä‡§Ç'\n",
      "\n",
      "Processed: ['5', \"'\", '100', '‡§Æ‡•á‡§Ç', '70', '‡§Ö‡§´‡§º‡§∏‡§∞', '‡§¨‡§®‡§®‡•á', '‡§≤‡§æ‡§Ø‡§ï‡§º', '‡§®‡§π‡•Ä‡§Ç', \"'\"]\n",
      "\n",
      "Original: 6\t\"100 ‡§∞‡•Å‡§™‡§Ø‡•á ‡§ï‡•á 8 ‡§ï‡§∞‡•ã‡§°‡§º ‡§ï‡•Ç‡§™‡§® ‡§õ‡§æ‡§™‡•á ‡§ú‡§æ‡§è‡§Å‡§ó‡•á.\n",
      "\n",
      "Processed: ['6', '\"', '100', '‡§∞‡•Å‡§™‡§Ø‡•á', '‡§ï‡•á', '8', '‡§ï‡§∞‡•ã‡§°‡§º', '‡§ï‡•Ç‡§™‡§®', '‡§õ‡§æ‡§™‡•á', '‡§ú‡§æ‡§è‡§Å‡§ó‡•á', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Hindi text\n",
    "def preprocess_hindi(text):\n",
    "    normalizer = IndicNormalizerFactory().get_normalizer(\"hi\") # Hindi language\n",
    "    text = normalizer.normalize(text) # Normalize the text\n",
    "    text = text.replace('\\n', ' ')  # Replace newlines with spaces\n",
    "    tokens = list(indic_tokenize.trivial_tokenize(text, lang='hi')) # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.strip()]  # Remove empty tokens\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "processed_hindi = [preprocess_hindi(sent) for sent in hindi_lines]\n",
    "\n",
    "# Example usage\n",
    "for i in range(2,6):\n",
    "    print(f\"Original: {hindi_lines[i]}\")\n",
    "    print(f\"Processed: {processed_hindi[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c622f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with at least 10 occurrences: 18158\n"
     ]
    }
   ],
   "source": [
    "# vocubulary size with words with a minimum frequence of 10 words\n",
    "# Flatten all tokens into a single list\n",
    "flat_words = [word for sentence in processed_hindi for word in sentence]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(flat_words)\n",
    "\n",
    "# Count words with at least 10 occurrences\n",
    "min_freq = 10\n",
    "num_words_10plus = sum(1 for count in word_counts.values() if count >= min_freq)\n",
    "\n",
    "print(f\"Number of words with at least {min_freq} occurrences: {num_words_10plus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6bb4b8",
   "metadata": {},
   "source": [
    "## Vocabulary Building with Frequency Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d574ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary with minimum frequency: 5...\n",
      "üìä Vocabulary Statistics:\n",
      "   Total unique words: 392891\n",
      "   Words with freq >= 5: 27123\n",
      "   Vocabulary reduction: 93.1%\n",
      "\n",
      "üî§ Sample vocabulary (first 20 words):\n",
      "['1', '03', '‡§Æ‡§ú‡§¶‡•Ç‡§∞‡•ã‡§Ç', '‡§ï‡•ã', '‡§¨‡•á‡§π‡§§‡§∞', '‡§á‡§≤‡§æ‡§ú', '‡§ï‡•á', '‡§≤‡§ø‡§è', '‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞', '‡§≤‡•á', '‡§ú‡§æ‡§®‡•á', '‡§ï‡•Ä', '‡§ï‡§∞‡§µ‡§æ‡§à', '‡§ó‡§à', '‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ', 'pic', '.', '2', '‚Ä¢', 'pm']\n",
      "\n",
      "üìà Most frequent words:\n",
      "   .: 231998\n",
      "   ‡§ï‡•á: 219853\n",
      "   ‡§Æ‡•á‡§Ç: 171219\n",
      "   ‡§π‡•à: 166705\n",
      "   ‡§ï‡•Ä: 139307\n",
      "   ‡§ï‡•ã: 102600\n",
      "   ‡§∏‡•á: 99867\n",
      "   ,: 94415\n",
      "   ‡§î‡§∞: 82439\n",
      "   ‡§®‡•á: 82413\n",
      "\n",
      "‚úÖ Vocabulary built with 27123 words\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(processed_hindi, min_freq=5):\n",
    "    print(f\"Building vocabulary with minimum frequency: {min_freq}...\")\n",
    "    \n",
    "    # Flatten all tokens into a single list\n",
    "    flat_words = [word for sentence in processed_hindi for word in sentence]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(flat_words)\n",
    "    \n",
    "    # Filter words with minimum frequency\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if count >= min_freq}\n",
    "    \n",
    "    # Create vocabulary from most frequent words\n",
    "    vocab = list(filtered_words.keys())\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    print(f\"üìä Vocabulary Statistics:\")\n",
    "    print(f\"   Total unique words: {len(word_counts)}\")\n",
    "    print(f\"   Words with freq >= {min_freq}: {vocab_size}\")\n",
    "    print(f\"   Vocabulary reduction: {(1 - vocab_size/len(word_counts))*100:.1f}%\")\n",
    "    \n",
    "    return vocab, word_counts, filtered_words\n",
    "\n",
    "# Build vocabulary\n",
    "vocab, word_counts, filtered_words = build_vocabulary(processed_hindi)\n",
    "print(f\"\\nüî§ Sample vocabulary (first 20 words):\")\n",
    "print(vocab[:20])\n",
    "\n",
    "print(f\"\\nüìà Most frequent words:\")\n",
    "for word, count in Counter(filtered_words).most_common(10):\n",
    "    print(f\"   {word}: {count}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Vocabulary built with {len(vocab)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e7f62",
   "metadata": {},
   "source": [
    "Now that we've built the vocabulary from the most frequent words, how can we assign meaningful IDs to these words instead of just arbitrary numbers?\n",
    "\n",
    "### There are several strategies we can use:\n",
    "\n",
    "# Frequency-based indexing ---> (most common and efficient)\n",
    "\n",
    "POS-based ordering ---> (useful for linguistic analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6809184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_mappings(vocab):\n",
    "    \"\"\"Create word-to-ID and ID-to-word mappings based on frequency\"\"\"\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(vocab)\n",
    "    \n",
    "    # Sort words by frequency (least frequent first)\n",
    "    sorted_vocab = sorted(word_freq, key=lambda word: word_freq[word])\n",
    "    \n",
    "    # Create mappings: word2id and id2word\n",
    "    word2id = {word: i for i, word in enumerate(sorted_vocab)}\n",
    "    id2word = {i: word for i, word in enumerate(sorted_vocab)}\n",
    "    \n",
    "    return word2id, id2word\n",
    "\n",
    "# Get word-to-ID and ID-to-word mappings\n",
    "word2id, id2word = create_word_mappings(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5adddb3",
   "metadata": {},
   "source": [
    "## Co-occurrence Matrix Construction with Window Size Experimentation\n",
    "\n",
    "We construct the co-occurrence matrix by counting how often word pairs appear within a context window, which can be adjusted based on experimentation. For each sentence, we update the matrix for every target-context word pair found within the specified window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f4ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_cooccurrence_matrix(sentences, word_to_idx, window_size=5):\n",
    "    \"\"\"Build co-occurrence matrix with specified window size for sample later we decide optimal window size\"\"\"\n",
    "    vocab_size = len(word_to_idx)\n",
    "    cooc_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for sentence in tqdm(sentences, desc=f\"Building co-occurrence matrix (window={window_size})\"):\n",
    "        # Filter words that are in vocabulary\n",
    "        valid_words = [word for word in sentence if word in word_to_idx]\n",
    "        \n",
    "        for i, target_word in enumerate(valid_words):\n",
    "            target_idx = word_to_idx[target_word]\n",
    "            \n",
    "            # Look at context words within window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(valid_words), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_word = valid_words[j]\n",
    "                    context_idx = word_to_idx[context_word]\n",
    "                    # Weight by distance (closer words get higher weight)\n",
    "                    distance = abs(i - j)\n",
    "                    weight = 1.0 / distance\n",
    "                    cooc_matrix[target_idx, context_idx] += weight\n",
    "    \n",
    "    return cooc_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351e858",
   "metadata": {},
   "source": [
    "### Window Size Analysis and Selection\n",
    "\n",
    "Small (2-5 words): Captures local relationships (syntax). \\\n",
    "Medium (5-10 words): Captures semantic meaning.\\\n",
    "Large (>10 words): Captures broader context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Experimenting with different window sizes...\n",
      "\n",
      "üìê Testing window size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [03:08<00:00, 1588.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 3,537,330\n",
      "   Sparsity: 0.9952\n",
      "   Density: 0.0048\n",
      "\n",
      "üìê Testing window size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [06:06<00:00, 817.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 6,445,326\n",
      "   Sparsity: 0.9912\n",
      "   Density: 0.0088\n",
      "\n",
      "üìê Testing window size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=5): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [07:13<00:00, 691.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 7,577,174\n",
      "   Sparsity: 0.9897\n",
      "   Density: 0.0103\n",
      "\n",
      "üìê Testing window size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=7): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [11:38<00:00, 429.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 9,397,390\n",
      "   Sparsity: 0.9872\n",
      "   Density: 0.0128\n",
      "\n",
      "üìê Testing window size: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=9): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [17:25<00:00, 286.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 10,772,734\n",
      "   Sparsity: 0.9854\n",
      "   Density: 0.0146\n",
      "\n",
      "üìê Testing window size: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=11): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [15:48<00:00, 316.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 11,826,031\n",
      "   Sparsity: 0.9839\n",
      "   Density: 0.0161\n",
      "\n",
      "üìê Testing window size: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=14): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [14:18<00:00, 349.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 12,970,329\n",
      "   Sparsity: 0.9824\n",
      "   Density: 0.0176\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Experiment with window sizes\u001b[39;00m\n\u001b[0;32m     64\u001b[0m WINDOW_SIZES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m14\u001b[39m]\n\u001b[1;32m---> 65\u001b[0m cooc_matrices, sparsity_df, optimal_window \u001b[38;5;241m=\u001b[39m analyze_window_sizes(\n\u001b[0;32m     66\u001b[0m     processed_hindi, word2id, WINDOW_SIZES\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Co-occurrence matrices built for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(WINDOW_SIZES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m window sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m, in \u001b[0;36manalyze_window_sizes\u001b[1;34m(sentences, word2id, window_sizes)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Density: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msparsity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Visualize sparsity analysis\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m df_sparsity \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sparsity_results)\n\u001b[0;32m     38\u001b[0m fig, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Sparsity vs Window Size\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_window_sizes(sentences, word2id, window_sizes=[2, 4, 5, 7, 9, 11, 14]):\n",
    "    \"\"\"Experiment with different window sizes and analyze sparsity\"\"\"\n",
    "    print(\"üî¨ Experimenting with different window sizes...\")\n",
    "    \n",
    "    cooc_matrices = {}\n",
    "    sparsity_results = []\n",
    "    \n",
    "    vocab_size = len(word2id)\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        print(f\"\\nüìê Testing window size: {window_size}\")\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        cooc_matrix = build_cooccurrence_matrix(sentences, word2id, window_size)\n",
    "        cooc_matrices[window_size] = cooc_matrix\n",
    "        \n",
    "        # Calculate sparsity\n",
    "        sparsity = 1 - cooc_matrix.nnz / (vocab_size * vocab_size)\n",
    "        non_zero_entries = cooc_matrix.nnz\n",
    "        \n",
    "        sparsity_results.append({\n",
    "            'window_size': window_size,\n",
    "            'sparsity': sparsity,\n",
    "            'non_zero_entries': non_zero_entries,\n",
    "            'density': 1 - sparsity\n",
    "        })\n",
    "        \n",
    "        print(f\"   Matrix shape: {cooc_matrix.shape}\")\n",
    "        print(f\"   Non-zero entries: {non_zero_entries:,}\")\n",
    "        print(f\"   Sparsity: {sparsity:.4f}\")\n",
    "        print(f\"   Density: {1-sparsity:.4f}\")\n",
    "    \n",
    "    # Visualize sparsity analysis\n",
    "    df_sparsity = pd.DataFrame(sparsity_results)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Sparsity vs Window Size\n",
    "    ax1.plot(df_sparsity['window_size'], df_sparsity['sparsity'], 'bo-', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Window Size')\n",
    "    ax1.set_ylabel('Sparsity')\n",
    "    ax1.set_title('Matrix Sparsity vs Window Size')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Non-zero entries vs Window Size\n",
    "    ax2.plot(df_sparsity['window_size'], df_sparsity['non_zero_entries'], 'ro-', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Window Size')\n",
    "    ax2.set_ylabel('Non-zero Entries')\n",
    "    ax2.set_title('Non-zero Entries vs Window Size')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select optimal window size (balance between sparsity and information)\n",
    "    optimal_window = 5  # Default choice\n",
    "    print(f\"\\nüéØ Selected optimal window size: {optimal_window}\")\n",
    "    \n",
    "    return cooc_matrices, df_sparsity, optimal_window\n",
    "\n",
    "# Experiment with window sizes\n",
    "WINDOW_SIZES = [2, 4, 5, 7, 9, 11, 14]\n",
    "cooc_matrices, sparsity_df, optimal_window = analyze_window_sizes(\n",
    "    processed_hindi, word2id, WINDOW_SIZES\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Co-occurrence matrices built for {len(WINDOW_SIZES)} window sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7c700",
   "metadata": {},
   "source": [
    "## evaluate with few word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90be95d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cooc_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Example: Assuming cooc_matrices, vocab, and word2id are defined somewhere in the code\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m window_analysis \u001b[38;5;241m=\u001b[39m analyze_window_size_quality(cooc_matrices, vocab, word2id)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Visualize window size analysis\u001b[39;00m\n\u001b[0;32m     45\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cooc_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def analyze_window_size_quality(cooc_matrices, vocab, word2id, sample_words=['king', 'queen', 'man', 'woman', 'good', 'bad']):\n",
    "    \"\"\"Analyze the quality of embeddings for different window sizes\"\"\"\n",
    "    results = {}\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    for window_size, matrix in cooc_matrices.items():\n",
    "        # Apply log transformation and normalize\n",
    "        log_matrix = matrix.copy().astype(np.float32)\n",
    "        log_matrix.data = np.log1p(log_matrix.data)  # log(1 + x)\n",
    "        \n",
    "        # Normalize rows\n",
    "        row_sums = np.array(log_matrix.sum(axis=1)).flatten()\n",
    "        row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "        log_matrix = log_matrix.multiply(1 / row_sums[:, np.newaxis])\n",
    "        \n",
    "        # Calculate average cosine similarity for sample word pairs\n",
    "        similarities = []\n",
    "        for i, word1 in enumerate(sample_words):\n",
    "            if word1 in word2id:\n",
    "                idx1 = word2id[word1]\n",
    "                vec1 = log_matrix.getrow(idx1).toarray().flatten()  # Use getrow to access row\n",
    "                \n",
    "                for word2 in sample_words[i+1:]:\n",
    "                    if word2 in word2id:\n",
    "                        idx2 = word2id[word2]\n",
    "                        vec2 = log_matrix.getrow(idx2).toarray().flatten()  # Use getrow to access row\n",
    "                        sim = cosine_similarity([vec1], [vec2])[0, 0]\n",
    "                        similarities.append(sim)\n",
    "        \n",
    "        # Compute results\n",
    "        results[window_size] = {\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'std_similarity': np.std(similarities),\n",
    "            'sparsity': 1 - matrix.nnz / (vocab_size * vocab_size)  # sparsity calculation\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Assuming cooc_matrices, vocab, and word2id are defined somewhere in the code\n",
    "window_analysis = analyze_window_size_quality(cooc_matrices, vocab, word2id)\n",
    "\n",
    "# Visualize window size analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "window_sizes_list = list(window_analysis.keys())\n",
    "avg_sims = [window_analysis[w]['avg_similarity'] for w in window_sizes_list]\n",
    "std_sims = [window_analysis[w]['std_similarity'] for w in window_sizes_list]\n",
    "sparsities = [window_analysis[w]['sparsity'] for w in window_sizes_list]\n",
    "\n",
    "axes[0].plot(window_sizes_list, avg_sims, 'o-')\n",
    "axes[0].set_title('Average Cosine Similarity')\n",
    "axes[0].set_xlabel('Window Size')\n",
    "axes[0].set_ylabel('Similarity')\n",
    "\n",
    "axes[1].plot(window_sizes_list, std_sims, 'o-', color='orange')\n",
    "axes[1].set_title('Similarity Standard Deviation')\n",
    "axes[1].set_xlabel('Window Size')\n",
    "axes[1].set_ylabel('Std Dev')\n",
    "\n",
    "axes[2].plot(window_sizes_list, sparsities, 'o-', color='green')\n",
    "axes[2].set_title('Matrix Sparsity')\n",
    "axes[2].set_xlabel('Window Size')\n",
    "axes[2].set_ylabel('Sparsity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best window size (balance between similarity and computational efficiency)\n",
    "best_window = 7  # Based on analysis\n",
    "print(f\"\\nSelected window size: {best_window}\")\n",
    "print(f\"Analysis results: {window_analysis[best_window]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3071998",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "To turn that matrix into word embeddings, you need to apply a technique like: \\\n",
    "\n",
    "Method -----> What it does \\\n",
    "# PCA / SVD -----> Reduce matrix to low-dimensional dense vec \n",
    "NMF (Non-negative Matrix Factorization) ---> Factorizes co-occurrence matrix into interpretable non-negatives \\\n",
    "GloVe -----> Uses the co-occurrence matrix to train word vectors \\\n",
    "word2vec -----> Learns embeddings directly via neural nets (skip-gram/CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca349f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8f0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbadcfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947d124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
