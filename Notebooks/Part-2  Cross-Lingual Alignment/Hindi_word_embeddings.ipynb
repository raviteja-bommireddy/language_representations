{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05aca86",
   "metadata": {},
   "source": [
    "# Hindi Word Embeddings: Complete Pipeline\n",
    "\n",
    "This notebook implements a complete pipeline for creating and evaluating English word embeddings using frequency-based co-occurrence matrices and PCA dimensionality reduction.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Data Preprocessing**: Text cleaning, tokenization, vocabulary building\n",
    "2. **Co-occurrence Matrix Construction**: Frequency-based approach with window size experimentation\n",
    "3. **Dimensionality Reduction**: PCA with dimension experimentation\n",
    "4. **Quantitative Evaluation**: Covariance, cosine similarity, analogies, clustering\n",
    "5. **Visualization**: t-SNE, PCA, similarity heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd85d2",
   "metadata": {},
   "source": [
    "## 🔧 Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb74a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting indic-nlp-library\n",
      "  Using cached indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\n",
      "  Using cached sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
      "  Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting morfessor (from indic-nlp-library)\n",
      "  Using cached Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from indic-nlp-library) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from indic-nlp-library) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->indic-nlp-library) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->indic-nlp-library) (2023.3)\n",
      "Requirement already satisfied: sphinx>=5.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx-argparse->indic-nlp-library) (7.3.7)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
      "  Using cached docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.15.1)\n",
      "Requirement already satisfied: snowballstemmer>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.11.0)\n",
      "Requirement already satisfied: alabaster~=0.7.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
      "Requirement already satisfied: imagesize>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
      "Requirement already satisfied: colorama>=0.4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.8.30)\n",
      "Using cached indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
      "Using cached Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Using cached sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
      "Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
      "Using cached docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Installing collected packages: morfessor, docutils, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
      "Successfully installed docutils-0.21.2 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts docutils.exe, rst2html.exe, rst2html4.exe, rst2html5.exe, rst2latex.exe, rst2man.exe, rst2odt.exe, rst2pseudoxml.exe, rst2s5.exe, rst2xetex.exe and rst2xml.exe are installed in 'C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3d6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indicnlp\n",
    "from indicnlp import common\n",
    "from indicnlp import loader\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45a24e",
   "metadata": {},
   "source": [
    "We will explore the generation of dense word representations from text corpora, analyze the quality of these representations. The goal is to understand how to represent word meaning in a high-dimensional space and how to transfer knowledge across languages.\n",
    "\n",
    "Please use the links below to download text corpora. \\\n",
    "English: https://wortschatz.uni-leipzig.de/en/download/English \\\n",
    "Hindi: https://wortschatz.uni-leipzig.de/en/download/Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6263f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_43680\\3395577974.py:7: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  path = \"D:\\PROJECTS\\PreCog\\Data\\hin_news_2020_300K\\hin_news_2020_300K-sentences.txt\"\n"
     ]
    }
   ],
   "source": [
    "# Set up Indic NLP resources\n",
    "INDIC_RESOURCES_PATH = \"D:\\\\RESEARCH related\\\\PreCog tasks\\\\indic_nlp_resources\"  # Replace with your path\n",
    "common.set_resources_path(INDIC_RESOURCES_PATH)\n",
    "loader.load()\n",
    "\n",
    "# Load Hindi corpus file\n",
    "path = \"D:\\PROJECTS\\PreCog\\Data\\hin_news_2020_300K\\hin_news_2020_300K-sentences.txt\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    hindi_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cabd97",
   "metadata": {},
   "source": [
    "## Preprocessing of data\n",
    "1. normalize\n",
    "2. replace new lines with spaces\n",
    "3. Remove non-alphabetic characters (removing punctuations)\n",
    "4. how about stop words removal --> they impact hugely in co occurence matrices so don't do it.\n",
    "5. lemitization & stemming  (does this helpful for this task)\n",
    "6. Tokenize words after all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2295c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 3\t० में कहा कि लॉकडाउन के बाद गरीब कल्याण योजना का ऐलान किया गया था।\n",
      "\n",
      "Processed: ['3', '०', 'में', 'कहा', 'कि', 'लॉकडाउन', 'के', 'बाद', 'गरीब', 'कल्याण', 'योजना', 'का', 'ऐलान', 'किया', 'गया', 'था', '।']\n",
      "\n",
      "Original: 4\t\"100 मरीजों पर नियंत्रित क्लिनिकल ट्रायल किया गया, जिसमें तीन दिन के अंदर 69 प्रतिशत और चार दिन के अंदर शत प्रतिशत मरीज ठीक हो गए और उनकी जांच रिपोर्ट निगेटिव आई।\"\n",
      "\n",
      "Processed: ['4', '\"', '100', 'मरीजों', 'पर', 'नियंत्रित', 'क्लिनिकल', 'ट्रायल', 'किया', 'गया', ',', 'जिसमें', 'तीन', 'दिन', 'के', 'अंदर', '69', 'प्रतिशत', 'और', 'चार', 'दिन', 'के', 'अंदर', 'शत', 'प्रतिशत', 'मरीज', 'ठीक', 'हो', 'गए', 'और', 'उनकी', 'जांच', 'रिपोर्ट', 'निगेटिव', 'आई', '।', '\"']\n",
      "\n",
      "Original: 5\t'100 में 70 अफ़सर बनने लायक़ नहीं'\n",
      "\n",
      "Processed: ['5', \"'\", '100', 'में', '70', 'अफ़सर', 'बनने', 'लायक़', 'नहीं', \"'\"]\n",
      "\n",
      "Original: 6\t\"100 रुपये के 8 करोड़ कूपन छापे जाएँगे.\n",
      "\n",
      "Processed: ['6', '\"', '100', 'रुपये', 'के', '8', 'करोड़', 'कूपन', 'छापे', 'जाएँगे', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Hindi text\n",
    "def preprocess_hindi(text):\n",
    "    normalizer = IndicNormalizerFactory().get_normalizer(\"hi\") # Hindi language\n",
    "    text = normalizer.normalize(text) # Normalize the text\n",
    "    text = text.replace('\\n', ' ')  # Replace newlines with spaces\n",
    "    tokens = list(indic_tokenize.trivial_tokenize(text, lang='hi')) # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.strip()]  # Remove empty tokens\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "processed_hindi = [preprocess_hindi(sent) for sent in hindi_lines]\n",
    "\n",
    "# Example usage\n",
    "for i in range(2,6):\n",
    "    print(f\"Original: {hindi_lines[i]}\")\n",
    "    print(f\"Processed: {processed_hindi[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c622f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with at least 10 occurrences: 18158\n"
     ]
    }
   ],
   "source": [
    "# vocubulary size with words with a minimum frequence of 10 words\n",
    "# Flatten all tokens into a single list\n",
    "flat_words = [word for sentence in processed_hindi for word in sentence]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(flat_words)\n",
    "\n",
    "# Count words with at least 10 occurrences\n",
    "min_freq = 10\n",
    "num_words_10plus = sum(1 for count in word_counts.values() if count >= min_freq)\n",
    "\n",
    "print(f\"Number of words with at least {min_freq} occurrences: {num_words_10plus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6bb4b8",
   "metadata": {},
   "source": [
    "## Vocabulary Building with Frequency Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d574ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary with minimum frequency: 5...\n",
      "📊 Vocabulary Statistics:\n",
      "   Total unique words: 392891\n",
      "   Words with freq >= 5: 27123\n",
      "   Vocabulary reduction: 93.1%\n",
      "\n",
      "🔤 Sample vocabulary (first 20 words):\n",
      "['1', '03', 'मजदूरों', 'को', 'बेहतर', 'इलाज', 'के', 'लिए', 'रायपुर', 'ले', 'जाने', 'की', 'करवाई', 'गई', 'व्यवस्था', 'pic', '.', '2', '•', 'pm']\n",
      "\n",
      "📈 Most frequent words:\n",
      "   .: 231998\n",
      "   के: 219853\n",
      "   में: 171219\n",
      "   है: 166705\n",
      "   की: 139307\n",
      "   को: 102600\n",
      "   से: 99867\n",
      "   ,: 94415\n",
      "   और: 82439\n",
      "   ने: 82413\n",
      "\n",
      "✅ Vocabulary built with 27123 words\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(processed_hindi, min_freq=5):\n",
    "    print(f\"Building vocabulary with minimum frequency: {min_freq}...\")\n",
    "    \n",
    "    # Flatten all tokens into a single list\n",
    "    flat_words = [word for sentence in processed_hindi for word in sentence]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(flat_words)\n",
    "    \n",
    "    # Filter words with minimum frequency\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if count >= min_freq}\n",
    "    \n",
    "    # Create vocabulary from most frequent words\n",
    "    vocab = list(filtered_words.keys())\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    print(f\"📊 Vocabulary Statistics:\")\n",
    "    print(f\"   Total unique words: {len(word_counts)}\")\n",
    "    print(f\"   Words with freq >= {min_freq}: {vocab_size}\")\n",
    "    print(f\"   Vocabulary reduction: {(1 - vocab_size/len(word_counts))*100:.1f}%\")\n",
    "    \n",
    "    return vocab, word_counts, filtered_words\n",
    "\n",
    "# Build vocabulary\n",
    "vocab, word_counts, filtered_words = build_vocabulary(processed_hindi)\n",
    "print(f\"\\n🔤 Sample vocabulary (first 20 words):\")\n",
    "print(vocab[:20])\n",
    "\n",
    "print(f\"\\n📈 Most frequent words:\")\n",
    "for word, count in Counter(filtered_words).most_common(10):\n",
    "    print(f\"   {word}: {count}\")\n",
    "\n",
    "print(f\"\\n✅ Vocabulary built with {len(vocab)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e7f62",
   "metadata": {},
   "source": [
    "Now that we've built the vocabulary from the most frequent words, how can we assign meaningful IDs to these words instead of just arbitrary numbers?\n",
    "\n",
    "### There are several strategies we can use:\n",
    "\n",
    "# Frequency-based indexing ---> (most common and efficient)\n",
    "\n",
    "POS-based ordering ---> (useful for linguistic analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6809184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_mappings(vocab):\n",
    "    \"\"\"Create word-to-ID and ID-to-word mappings based on frequency\"\"\"\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(vocab)\n",
    "    \n",
    "    # Sort words by frequency (least frequent first)\n",
    "    sorted_vocab = sorted(word_freq, key=lambda word: word_freq[word])\n",
    "    \n",
    "    # Create mappings: word2id and id2word\n",
    "    word2id = {word: i for i, word in enumerate(sorted_vocab)}\n",
    "    id2word = {i: word for i, word in enumerate(sorted_vocab)}\n",
    "    \n",
    "    return word2id, id2word\n",
    "\n",
    "# Get word-to-ID and ID-to-word mappings\n",
    "word2id, id2word = create_word_mappings(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5adddb3",
   "metadata": {},
   "source": [
    "## Co-occurrence Matrix Construction with Window Size Experimentation\n",
    "\n",
    "We construct the co-occurrence matrix by counting how often word pairs appear within a context window, which can be adjusted based on experimentation. For each sentence, we update the matrix for every target-context word pair found within the specified window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f4ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_cooccurrence_matrix(sentences, word_to_idx, window_size=5):\n",
    "    \"\"\"Build co-occurrence matrix with specified window size for sample later we decide optimal window size\"\"\"\n",
    "    vocab_size = len(word_to_idx)\n",
    "    cooc_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for sentence in tqdm(sentences, desc=f\"Building co-occurrence matrix (window={window_size})\"):\n",
    "        # Filter words that are in vocabulary\n",
    "        valid_words = [word for word in sentence if word in word_to_idx]\n",
    "        \n",
    "        for i, target_word in enumerate(valid_words):\n",
    "            target_idx = word_to_idx[target_word]\n",
    "            \n",
    "            # Look at context words within window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(valid_words), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_word = valid_words[j]\n",
    "                    context_idx = word_to_idx[context_word]\n",
    "                    # Weight by distance (closer words get higher weight)\n",
    "                    distance = abs(i - j)\n",
    "                    weight = 1.0 / distance\n",
    "                    cooc_matrix[target_idx, context_idx] += weight\n",
    "    \n",
    "    return cooc_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351e858",
   "metadata": {},
   "source": [
    "### Window Size Analysis and Selection\n",
    "\n",
    "Small (2-5 words): Captures local relationships (syntax). \\\n",
    "Medium (5-10 words): Captures semantic meaning.\\\n",
    "Large (>10 words): Captures broader context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Experimenting with different window sizes...\n",
      "\n",
      "📐 Testing window size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=2): 100%|██████████| 300000/300000 [03:08<00:00, 1588.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 3,537,330\n",
      "   Sparsity: 0.9952\n",
      "   Density: 0.0048\n",
      "\n",
      "📐 Testing window size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=4): 100%|██████████| 300000/300000 [06:06<00:00, 817.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 6,445,326\n",
      "   Sparsity: 0.9912\n",
      "   Density: 0.0088\n",
      "\n",
      "📐 Testing window size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=5): 100%|██████████| 300000/300000 [07:13<00:00, 691.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 7,577,174\n",
      "   Sparsity: 0.9897\n",
      "   Density: 0.0103\n",
      "\n",
      "📐 Testing window size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=7): 100%|██████████| 300000/300000 [11:38<00:00, 429.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 9,397,390\n",
      "   Sparsity: 0.9872\n",
      "   Density: 0.0128\n",
      "\n",
      "📐 Testing window size: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=9): 100%|██████████| 300000/300000 [17:25<00:00, 286.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 10,772,734\n",
      "   Sparsity: 0.9854\n",
      "   Density: 0.0146\n",
      "\n",
      "📐 Testing window size: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=11): 100%|██████████| 300000/300000 [15:48<00:00, 316.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 11,826,031\n",
      "   Sparsity: 0.9839\n",
      "   Density: 0.0161\n",
      "\n",
      "📐 Testing window size: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix (window=14): 100%|██████████| 300000/300000 [14:18<00:00, 349.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matrix shape: (27123, 27123)\n",
      "   Non-zero entries: 12,970,329\n",
      "   Sparsity: 0.9824\n",
      "   Density: 0.0176\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Experiment with window sizes\u001b[39;00m\n\u001b[0;32m     64\u001b[0m WINDOW_SIZES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m14\u001b[39m]\n\u001b[1;32m---> 65\u001b[0m cooc_matrices, sparsity_df, optimal_window \u001b[38;5;241m=\u001b[39m analyze_window_sizes(\n\u001b[0;32m     66\u001b[0m     processed_hindi, word2id, WINDOW_SIZES\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Co-occurrence matrices built for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(WINDOW_SIZES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m window sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m, in \u001b[0;36manalyze_window_sizes\u001b[1;34m(sentences, word2id, window_sizes)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Density: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msparsity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Visualize sparsity analysis\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m df_sparsity \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sparsity_results)\n\u001b[0;32m     38\u001b[0m fig, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Sparsity vs Window Size\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_window_sizes(sentences, word2id, window_sizes=[2, 4, 5, 7, 9, 11, 14]):\n",
    "    \"\"\"Experiment with different window sizes and analyze sparsity\"\"\"\n",
    "    print(\"🔬 Experimenting with different window sizes...\")\n",
    "    \n",
    "    cooc_matrices = {}\n",
    "    sparsity_results = []\n",
    "    \n",
    "    vocab_size = len(word2id)\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        print(f\"\\n📐 Testing window size: {window_size}\")\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        cooc_matrix = build_cooccurrence_matrix(sentences, word2id, window_size)\n",
    "        cooc_matrices[window_size] = cooc_matrix\n",
    "        \n",
    "        # Calculate sparsity\n",
    "        sparsity = 1 - cooc_matrix.nnz / (vocab_size * vocab_size)\n",
    "        non_zero_entries = cooc_matrix.nnz\n",
    "        \n",
    "        sparsity_results.append({\n",
    "            'window_size': window_size,\n",
    "            'sparsity': sparsity,\n",
    "            'non_zero_entries': non_zero_entries,\n",
    "            'density': 1 - sparsity\n",
    "        })\n",
    "        \n",
    "        print(f\"   Matrix shape: {cooc_matrix.shape}\")\n",
    "        print(f\"   Non-zero entries: {non_zero_entries:,}\")\n",
    "        print(f\"   Sparsity: {sparsity:.4f}\")\n",
    "        print(f\"   Density: {1-sparsity:.4f}\")\n",
    "    \n",
    "    # Visualize sparsity analysis\n",
    "    df_sparsity = pd.DataFrame(sparsity_results)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Sparsity vs Window Size\n",
    "    ax1.plot(df_sparsity['window_size'], df_sparsity['sparsity'], 'bo-', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Window Size')\n",
    "    ax1.set_ylabel('Sparsity')\n",
    "    ax1.set_title('Matrix Sparsity vs Window Size')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Non-zero entries vs Window Size\n",
    "    ax2.plot(df_sparsity['window_size'], df_sparsity['non_zero_entries'], 'ro-', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Window Size')\n",
    "    ax2.set_ylabel('Non-zero Entries')\n",
    "    ax2.set_title('Non-zero Entries vs Window Size')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select optimal window size (balance between sparsity and information)\n",
    "    optimal_window = 5  # Default choice\n",
    "    print(f\"\\n🎯 Selected optimal window size: {optimal_window}\")\n",
    "    \n",
    "    return cooc_matrices, df_sparsity, optimal_window\n",
    "\n",
    "# Experiment with window sizes\n",
    "WINDOW_SIZES = [2, 4, 5, 7, 9, 11, 14]\n",
    "cooc_matrices, sparsity_df, optimal_window = analyze_window_sizes(\n",
    "    processed_hindi, word2id, WINDOW_SIZES\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Co-occurrence matrices built for {len(WINDOW_SIZES)} window sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7c700",
   "metadata": {},
   "source": [
    "## evaluate with few word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90be95d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cooc_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Example: Assuming cooc_matrices, vocab, and word2id are defined somewhere in the code\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m window_analysis \u001b[38;5;241m=\u001b[39m analyze_window_size_quality(cooc_matrices, vocab, word2id)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Visualize window size analysis\u001b[39;00m\n\u001b[0;32m     45\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cooc_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def analyze_window_size_quality(cooc_matrices, vocab, word2id, sample_words=['king', 'queen', 'man', 'woman', 'good', 'bad']):\n",
    "    \"\"\"Analyze the quality of embeddings for different window sizes\"\"\"\n",
    "    results = {}\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    for window_size, matrix in cooc_matrices.items():\n",
    "        # Apply log transformation and normalize\n",
    "        log_matrix = matrix.copy().astype(np.float32)\n",
    "        log_matrix.data = np.log1p(log_matrix.data)  # log(1 + x)\n",
    "        \n",
    "        # Normalize rows\n",
    "        row_sums = np.array(log_matrix.sum(axis=1)).flatten()\n",
    "        row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "        log_matrix = log_matrix.multiply(1 / row_sums[:, np.newaxis])\n",
    "        \n",
    "        # Calculate average cosine similarity for sample word pairs\n",
    "        similarities = []\n",
    "        for i, word1 in enumerate(sample_words):\n",
    "            if word1 in word2id:\n",
    "                idx1 = word2id[word1]\n",
    "                vec1 = log_matrix.getrow(idx1).toarray().flatten()  # Use getrow to access row\n",
    "                \n",
    "                for word2 in sample_words[i+1:]:\n",
    "                    if word2 in word2id:\n",
    "                        idx2 = word2id[word2]\n",
    "                        vec2 = log_matrix.getrow(idx2).toarray().flatten()  # Use getrow to access row\n",
    "                        sim = cosine_similarity([vec1], [vec2])[0, 0]\n",
    "                        similarities.append(sim)\n",
    "        \n",
    "        # Compute results\n",
    "        results[window_size] = {\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'std_similarity': np.std(similarities),\n",
    "            'sparsity': 1 - matrix.nnz / (vocab_size * vocab_size)  # sparsity calculation\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Assuming cooc_matrices, vocab, and word2id are defined somewhere in the code\n",
    "window_analysis = analyze_window_size_quality(cooc_matrices, vocab, word2id)\n",
    "\n",
    "# Visualize window size analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "window_sizes_list = list(window_analysis.keys())\n",
    "avg_sims = [window_analysis[w]['avg_similarity'] for w in window_sizes_list]\n",
    "std_sims = [window_analysis[w]['std_similarity'] for w in window_sizes_list]\n",
    "sparsities = [window_analysis[w]['sparsity'] for w in window_sizes_list]\n",
    "\n",
    "axes[0].plot(window_sizes_list, avg_sims, 'o-')\n",
    "axes[0].set_title('Average Cosine Similarity')\n",
    "axes[0].set_xlabel('Window Size')\n",
    "axes[0].set_ylabel('Similarity')\n",
    "\n",
    "axes[1].plot(window_sizes_list, std_sims, 'o-', color='orange')\n",
    "axes[1].set_title('Similarity Standard Deviation')\n",
    "axes[1].set_xlabel('Window Size')\n",
    "axes[1].set_ylabel('Std Dev')\n",
    "\n",
    "axes[2].plot(window_sizes_list, sparsities, 'o-', color='green')\n",
    "axes[2].set_title('Matrix Sparsity')\n",
    "axes[2].set_xlabel('Window Size')\n",
    "axes[2].set_ylabel('Sparsity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best window size (balance between similarity and computational efficiency)\n",
    "best_window = 7  # Based on analysis\n",
    "print(f\"\\nSelected window size: {best_window}\")\n",
    "print(f\"Analysis results: {window_analysis[best_window]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3071998",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "To turn that matrix into word embeddings, you need to apply a technique like: \\\n",
    "\n",
    "Method -----> What it does \\\n",
    "# PCA / SVD -----> Reduce matrix to low-dimensional dense vec \n",
    "NMF (Non-negative Matrix Factorization) ---> Factorizes co-occurrence matrix into interpretable non-negatives \\\n",
    "GloVe -----> Uses the co-occurrence matrix to train word vectors \\\n",
    "word2vec -----> Learns embeddings directly via neural nets (skip-gram/CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca349f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8f0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbadcfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947d124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
